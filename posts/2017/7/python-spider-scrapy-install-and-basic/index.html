<!DOCTYPE html>
<html lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="yokonsan" />
	
	
	
	<title>Python爬虫(11):Scrapy框架的安装和基本使用 ｜ 乾之 三爻</title>
	
    
    
    <meta name="description" content="大家好，本篇文章我们来看一下强大的Python爬虫框架Scrapy。Scrapy是一个使用简单，功能强大的异步爬虫框架，我们先来看看他的安装。 Scrapy的安装 Scrapy的安装是很麻烦的，对于一些想" />
    

    
    
    <meta name="keywords" content="技术, 生活, 分享" />
    

	
    
    <link rel="shortcut icon" href="https://yokonsan.com/images/favicon.ico" />

    <link rel="stylesheet" type="text/css" media="screen" href="https://yokonsan.com/css/normalize.css" />
    <link rel="stylesheet" type="text/css" media="screen" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.0/animate.min.css" />
    <link rel="stylesheet" type="text/css" media="screen" href="https://yokonsan.com/css/zozo.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" />
    <link rel="stylesheet" type="text/css" media="screen" href="https://yokonsan.com/css/highlight.css" />

    
    
</head>

<body>
    <div class="main animate__animated animate__fadeInDown">
        <div class="nav_container animated fadeInDown">
    <div class="site_nav" id="site_nav">
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/posts/">Archive</a>
            </li>
            
            <li>
                <a href="/tags/">Tags</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
        </ul>
    </div>
    <div class="menu_icon">
        <a id="menu_icon"><i class="ri-menu-line"></i></a>
    </div>
</div>
        <div class="header animated fadeInDown">
    <div class="site_title_container">
        <div class="site_title">
            <h1>
                <a href="https://yokonsan.com/">
                    <span>乾之 三爻</span>
                </a>
            </h1>
        </div>
        <div class="description">
            <p class="sub_title">yokon&#39;s blog</p>
            <div class="my_socials">
                
                
                <a href="https://github.com/yokonsan" title="github" target="_blank"><i class="ri-github-fill"></i></a>
                
                
                <a href="https://yokonsan.com/index.xml" type="application/rss+xml" title="rss" target="_blank"><i
                        class="ri-rss-fill"></i></a>
            </div>
        </div>
    </div>
</div>
        <div class="content">
            <div class="post_page">
                <div class="post animate__animated animate__fadeInDown">
                    <div class="post_title post_detail_title">
                        <h2><a href='/posts/2017/7/python-spider-scrapy-install-and-basic/'>Python爬虫(11):Scrapy框架的安装和基本使用</a></h2>
                        <span class="date">2017.07.27</span>
                    </div>
                    <div class="post_content markdown"><p>大家好，本篇文章我们来看一下强大的<code>Python</code>爬虫框架<code>Scrapy</code>。<code>Scrapy</code>是一个使用简单，功能强大的异步爬虫框架，我们先来看看他的安装。</p>
<h2 id="scrapy的安装">Scrapy的安装</h2>
<p><code>Scrapy</code>的安装是很麻烦的，对于一些想使用<code>Scrapy</code>的人来说，它的安装常常就让很多人死在半路。在此我将我的安装过程和网络上整理的安装方法，分享给大家，希望大家能够安装顺利。</p>
<h3 id="windows安装">Windows安装</h3>
<p>开始之前，我们要确定自己安装了<code>Python</code>，本篇文章我们以<code>Python3.5</code>为例。<code>Scrapy</code>有很多依赖的包，我们来一一安装。</p>
<ul>
<li>首先，使用<code>pip -v</code>，查看<code>pip</code>是否安装正常，如果正常，那么我们进行下一步；</li>
<li><code>pip install wheel</code>这个包我们之前的文章介绍过，安装好他我们就可以安装一些<code>wheel</code>件；</li>
<li><code>lxml</code>安装，之前的文章说过他的安装，那么我们这里在重新整理一下。whl文件地址：<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml">here</a>。找到自己对应版本的文件，下载好后，找到文件位置，右键点击文件属性，点击安全标签，复制他的所在路径。打开管理员工具(cmd)，<code>pip install &lt;粘贴whl路径&gt;</code>；</li>
<li><code>PyOpenssl</code> 的<code>whl</code>文件地址：<a href="http://www.python.org/pypi/pyOpenSSL#downloads">here</a>。点击下载，<code>whl</code>文件安装方式同上；</li>
<li><code>Twisted</code>框架这个框架是一个异步网络库，是<code>Scrapy</code>的核心。<code>whl</code>文件地址：<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted">here</a>；</li>
<li><code>Pywin32</code>这是一个<code>Pywin32</code>兼容的库，下载地址：<a href="https://sourceforge.net/projects/pywin32/files/pywon32/Build%20220">here</a>，选好版本进行下载；</li>
<li>如果上面的库全都安装好了，那么我们就可以安装我们的<code>Scrapy</code>了，<code>pip install scrapy</code></li>
</ul>
<p>是不是很麻烦呢，如果大家不喜欢折腾，那么在<code>Windows</code>下也可以很方便的安装。那就要使用我们之前提到的<code>Anaconda</code>了。具体安装大家自己找找，或者在之前的文章中找。那么他的安装<code>Scrapy</code>只需要一行：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">conda install scrapy
</span></span></code></pre></div><h3 id="linux安装">Linux安装</h3>
<p><code>Linux</code>系统安装起来就要简单一点：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">sudo apt-get install build-essential python3-dev libssl-dev libffi-dev libxml2 libxml2-dev libxslt1-dev zlib1g-dev
</span></span></code></pre></div><h3 id="mac-os安装">Mac OS安装</h3>
<p>我们需要先安装一些<code>C++</code>的依赖库，<code>xcode-select --install</code></p>
<p>需要安装命令行开发工具，我们点击安装。安装完成，那么依赖库也就安装完成了。</p>
<p>然后我们直接使用<code>pip</code>安装<code>pip install scrapy</code></p>
<p>以上，我们的<code>Scrapy</code>库的安装基本上就解决了。</p>
<h2 id="scrapy的基本使用">Scrapy的基本使用</h2>
<p><code>Scrapy</code>的中文文档地址：<a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/overview.html">here</a></p>
<blockquote>
<p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p>
</blockquote>
<p>他的基本项目流程为：</p>
<ul>
<li>创建一个<code>Scrapy</code>项目</li>
<li>定义提取的<code>Item</code></li>
<li>编写爬取网站的<code>spider</code>并提取<code>Item</code></li>
<li>编写<code>Item Pipeline</code>来存储提取到的<code>Item</code>(即数据)</li>
</ul>
<p>而一般我们的爬虫流程为：</p>
<ul>
<li>抓取索引页：请求索引页的<code>URL</code>并得到源代码，进行下一步分析；</li>
<li>获取内容和下一页链接：分析源代码，提取索引页数据，并且获取下一页链接，进行下一步抓取；</li>
<li>翻页爬取：请求下一页信息，分析内容并请求在下一页链接；</li>
<li>保存爬取结果：将爬取结果保存为特定格式和文本，或者保存数据库。</li>
</ul>
<p>我们一步一步来看看如何使用。</p>
<h3 id="创建项目">创建项目</h3>
<p>在开始爬取之前，您必须创建一个新的<code>Scrapy</code>项目。 进入您打算存储代码的目录中，运行下列命令（以知乎日报为例）:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">scrapy startproject zhihurb
</span></span></code></pre></div><p>该命令将会创建包含下列内容的 zhihu 目录:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">zhihurb/
</span></span><span class="line"><span class="cl">	scrapy.cfg
</span></span><span class="line"><span class="cl">	zhihurb/
</span></span><span class="line"><span class="cl">		__init__.py
</span></span><span class="line"><span class="cl">		items.py
</span></span><span class="line"><span class="cl">		pipelines.py
</span></span><span class="line"><span class="cl">		settings.py
</span></span><span class="line"><span class="cl">		spiders/
</span></span><span class="line"><span class="cl">			__init__.py
</span></span><span class="line"><span class="cl">			...
</span></span></code></pre></div><p>这些文件分别是:</p>
<blockquote>
<p>scrapy.cfg: 项目的配置文件</p>
</blockquote>
<blockquote>
<p>zhihurb/: 该项目的python模块。之后您将在此加入代码。</p>
</blockquote>
<blockquote>
<p>zhihurb/items.py: 项目中的item文件.</p>
</blockquote>
<blockquote>
<p>zhihurb/pipelines.py: 项目中的pipelines文件.</p>
</blockquote>
<blockquote>
<p>zhihurb/settings.py: 项目的设置文件.</p>
</blockquote>
<blockquote>
<p>zhihurb/spiders/: 放置spider代码的目录.</p>
</blockquote>
<h3 id="定义item">定义Item</h3>
<p>这一步是定义我们需要获取到的数据信息，比如我们需要获得网站里的一些<code>url</code>，网站文章的内容，文章的作者等。这一步定义的地方就在我们的<code>items.py</code>文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">scrapy</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">ZhihuItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">	<span class="n">article</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</span></span></code></pre></div><h3 id="编写spider">编写Spider</h3>
<p>这一步就是写我们最熟悉的爬虫了，而我们的<code>Scrapy</code>框架可以让我们不需要去考虑实现的方法，只需要写出爬取的逻辑就可以了。</p>
<p>首先我们需要在 spiders/ 文件夹下创建我们的爬虫文件，比如就叫<code>spider.py</code>。写爬虫前，我们需要先定义一些内容。我们以知乎日报为例：<code>https://daily.zhihu.com/</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Spider</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">ZhihuSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="n">name</span> <span class="o">=</span> <span class="s2">&#34;zhihu&#34;</span>
</span></span><span class="line"><span class="cl">	<span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;zhihu.com&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">	<span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;https://daily.zhihu.com/&#39;</span><span class="p">]</span>
</span></span></code></pre></div><p>这里我们定义了什么呢?首先我们导入了<code>Scrapy</code>的<code>Spider</code>组件。然后创建一个爬虫类，在类里我们定义了我们的爬虫名称：zhihu（注意：爬虫名称独一无二的，是不可以和别的爬虫重复的）。还定义了一个网址范围，和一个起始 url 列表，说明起始 url 可以是多个。</p>
<p>然后我们定义一个解析函数：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</span></span></code></pre></div><p>我们直接打印一下，看看这个解析函数是什么。</p>
<h3 id="运行爬虫">运行爬虫</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">scrapy crawl zhihu
</span></span></code></pre></div><p>由于<code>Scrapy</code>是不支持在<code>IDE</code>中执行，所以我们必须在命令行里执行命令，我们要确定是不是<code>cd</code>到爬虫目录下。然后执行，这里的命令顾名思义，<code>crawl</code>是蜘蛛的意思，<code>zhihu</code>就是我们定义的爬虫名称了。</p>
<p>查看输出，我们先看到的是一些爬虫类的输出，可以看到输出的<code>log</code>中包含定义在 <code>start_urls</code> 的初始URL，并且与<code>spider</code>中是一一对应的。我们接着可以看到打印出了网页源代码。可是我们似乎并没有做什么，就得到了网页的源码，这是<code>Scrapy</code>比较方便的一点。</p>
<h3 id="提取数据">提取数据</h3>
<p>接着就可以使用解析工具解析源码，拿到数据了。</p>
<p>由于<code>Scrapy</code>内置了<code>CSS</code>和<code>xpath</code>选择器，而我们虽然可以使用<code>Beautifulsoup</code>，但是<code>BeautifulSoup</code>的缺点就是慢，这不符合我们<code>Scrapy</code>的风格，所有我还是建议大家使用<code>CSS</code>或者<code>Xpath</code>。</p>
<p>由于之前我并没有写过关于<code>Xpath</code>或者<code>CSS</code>选择器的用法，那么首先这个并不难，而且熟悉浏览器的用法，可以很简单的掌握他们。</p>
<p>我们以提取知乎日报里的文章<code>url</code>为例：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Request</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="n">urls</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@class=&#34;box&#34;]/a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">		<span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_url</span><span class="p">)</span>
</span></span></code></pre></div><p>这里我们使用<code>xpath</code>解析出所有的<code>url</code>（extract()是获得所有URL集合，extract_first()是获得第一个）。然后将<code>url</code>利用<code>yield</code>语法糖，回调函数给下一个解析<code>url</code>的函数。</p>
<h3 id="使用item">使用item</h3>
<p>后面详细的组件使用留在下一章讲解，这里假如我们解析出了文章内容和标题，我们要将提取的数据保存到<code>item</code>容器。</p>
<p><code>Item </code>对象相当于是自定义的<code>python</code>字典。 您可以使用标准的字典语法来获取到其每个字段的值。(字段即是我们之前用Field赋值的属性)。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 假如我们下一个解析函数解析出了数据</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">parse_url</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="c1"># name = xxxx</span>
</span></span><span class="line"><span class="cl">	<span class="c1"># article = xxxx</span>
</span></span><span class="line"><span class="cl">	<span class="c1"># 保存</span>
</span></span><span class="line"><span class="cl">	<span class="n">item</span> <span class="o">=</span> <span class="n">DmozItem</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">	<span class="n">item</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span>
</span></span><span class="line"><span class="cl">	<span class="n">item</span><span class="p">[</span><span class="s1">&#39;article&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">article</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1"># 返回item</span>
</span></span><span class="line"><span class="cl">	<span class="k">yield</span> <span class="n">item</span>
</span></span></code></pre></div><h3 id="保存爬取到的数据">保存爬取到的数据</h3>
<p>这里我们需要在管道文件<code>pipelines.py</code>里去操作数据，比如我们要将这些数据的文章标题只保留 5 个字，然后保存在文本里。或者我们要将数据保存到数据库里，这些都是在管道文件里面操作。我们后面在详细讲解。</p>
<p>那么最简单的存储方法是使用命令行命令：</p>
<p><code>scrapy crawl zhihu -o items.json</code></p>
<p>这条命令就会完成我们的数据保存在根目录的<code>json</code>文件里，我们还可以将他格式保存为<code>msv</code>,<code>pickle</code>等。改变命令后面的格式就可以了。</p>
<h2 id="最后">最后</h2>
<p>本篇教程仅介绍了<code>Scrapy</code>的基础，还有很多特性没有涉及到，那么我会在下一篇文章分享一下我对于<code>Scrapy</code>组件的学习理解。</p>
<p>谢谢阅读</p>
</div>
                    <div class="post_footer">
                        
                        <div class="meta">
                            <div class="info">
                                <span class="field tags">
                                    <i class="ri-stack-line"></i>
                                    
                                    <a href="https://yokonsan.com/tags/python/">Python</a>
                                    
                                    <a href="https://yokonsan.com/tags/%E7%88%AC%E8%99%AB/">爬虫</a>
                                    
                                </span>
                            </div>
                        </div>
                        
                    </div>
                </div>
                
                
                <div class="doc_comments"></div>
                
            </div>
        </div>
    </div>
    <a id="back_to_top" href="#" class="back_to_top"><i class="ri-arrow-up-s-line"></i></a>
    <footer class="footer">
    <div class="powered_by">
        <a href="https://github.com/varkai/hugo-theme-zozo">Designed by zozo,</a>
        <a href="http://www.gohugo.io/">Proudly published with Hugo</a>
    </div>

    <div class="footer_slogan">
        <span>博观约取，厚积薄发</span>
    </div>
</footer>
    <script src="https://yokonsan.com/js/jquery-3.5.1.min.js"></script>
<link href="https://yokonsan.com/css/fancybox.min.css" rel="stylesheet">
<script src="https://yokonsan.com/js/fancybox.min.js"></script>
<script src="https://yokonsan.com/js/zozo.js"></script>


<script type="text/javascript" async
    src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\[\[', '\]\]']],
                processEscapes: true,
                processEnvironments: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
                TeX: {
                    equationNumbers: { autoNumber: "AMS" },
                    extensions: ["AMSmath.js", "AMSsymbols.js"]
                }
            }
        });

        MathJax.Hub.Queue(function () {
            
            
            
            var all = MathJax.Hub.getAllJax(), i;
            for (i = 0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
            }
        });
    </script>

<style>
    code.has-jax {
        font: inherit;
        font-size: 100%;
        background: inherit;
        border: inherit;
        color: #515151;
    }
</style>



</body>

</html>